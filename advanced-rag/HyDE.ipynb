{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from langchain.llms import WatsonxLLM\n",
    "from langchain.chains import HypotheticalDocumentEmbedder\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import langchain\n",
    "import ast\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    TextGenerationParameters,\n",
    ")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bam_model(model_id='mistralai/mixtral-8x7b-instruct-v0-1', decoding_method='greedy', max_new_tokens=1000, \n",
    "              min_new_tokens=1, temperature=0.5, top_k=50, top_p=1, repetition_penalty=1):\n",
    "\n",
    "    if decoding_method == 'greedy':\n",
    "        decoding_method = DecodingMethod.GREEDY\n",
    "        parameters=TextGenerationParameters(\n",
    "            decoding_method=decoding_method,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            min_new_tokens=min_new_tokens,\n",
    "            repetition_penalty=repetition_penalty\n",
    "        )\n",
    "    else:\n",
    "        decoding_method = DecodingMethod.SAMPLE\n",
    "        parameters=TextGenerationParameters(\n",
    "            decoding_method=decoding_method,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            min_new_tokens=min_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty\n",
    "        )\n",
    "\n",
    "    llm = LangChainInterface(\n",
    "        model_id=model_id,\n",
    "        client=Client(credentials=Credentials.from_env()),\n",
    "        parameters=parameters,\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walgreens store sales average</td>\n",
       "      <td>[The average Walgreens salary ranges from appr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how much do bartenders make</td>\n",
       "      <td>[A bartender’s income is comprised mostly of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is a furuncle boil</td>\n",
       "      <td>[Knowledge center. A boil, also known as a fur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what can urinalysis detect</td>\n",
       "      <td>[Urinalysis: One way to test for bladder cance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is vitamin a used for</td>\n",
       "      <td>[Since vitamin A is fat-soluble it is not need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>who wrote Nothing compares to you</td>\n",
       "      <td>[Nothing Compares 2 U  is a song originally wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>cost for relining dentures</td>\n",
       "      <td>[When dentures that used to fit are now loose ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>what is sherry wine made of</td>\n",
       "      <td>[Sherry vinegar is made from sherry, a fortifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>dna in bacteria</td>\n",
       "      <td>[Bacterial DNA in Human Genomes. A new study f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>primary secondary and tertiary sectors definition</td>\n",
       "      <td>[The movement of goods and services through th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0                        walgreens store sales average   \n",
       "1                          how much do bartenders make   \n",
       "2                              what is a furuncle boil   \n",
       "3                           what can urinalysis detect   \n",
       "4                           what is vitamin a used for   \n",
       "..                                                 ...   \n",
       "195                  who wrote Nothing compares to you   \n",
       "196                         cost for relining dentures   \n",
       "197                        what is sherry wine made of   \n",
       "198                                    dna in bacteria   \n",
       "199  primary secondary and tertiary sectors definition   \n",
       "\n",
       "                                              contexts  \n",
       "0    [The average Walgreens salary ranges from appr...  \n",
       "1    [A bartender’s income is comprised mostly of t...  \n",
       "2    [Knowledge center. A boil, also known as a fur...  \n",
       "3    [Urinalysis: One way to test for bladder cance...  \n",
       "4    [Since vitamin A is fat-soluble it is not need...  \n",
       "..                                                 ...  \n",
       "195  [Nothing Compares 2 U  is a song originally wr...  \n",
       "196  [When dentures that used to fit are now loose ...  \n",
       "197  [Sherry vinegar is made from sherry, a fortifi...  \n",
       "198  [Bacterial DNA in Human Genomes. A new study f...  \n",
       "199  [The movement of goods and services through th...  \n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ms-marco.csv\")\n",
    "df[\"contexts\"] = [ast.literal_eval(ctx) for ctx in df[\"contexts\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_chunks(texts: str,\n",
    "                   chunk_length: int = 100,\n",
    "                   chunk_overlap: int = 25) -> list:\n",
    "    \"\"\"\n",
    "    Splits the text into equally distributed chunks with 25-word overlap.\n",
    "    Args:\n",
    "        texts (str): Text to be converted into chunks.\n",
    "        chunk_length (int): Maximum number of words in each chunk.\n",
    "        chunk_overlap (int): Number of words to overlap between chunks.\n",
    "    \"\"\"\n",
    "    words = texts.split(' ')\n",
    "    n = len(words)\n",
    "    chunks = []\n",
    "    chunk_number = 1\n",
    "    i = 0\n",
    "    while i < n:  # Corrected the length check\n",
    "        chunk = words[i: min(i + chunk_length, n)]\n",
    "        i = i + chunk_length - chunk_overlap\n",
    "        #print(len(chunk))\n",
    "        chunk = ' '.join(chunk).strip()\n",
    "        chunks.append({\"text\": chunk})\n",
    "        chunk_number += 1\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager. Salary information comes from 7,810 data points collected directly from employees, users, and jobs on Indeed.The average revenue in 2011 of a Starbuck Store was $1,078,000, up  from $1,011,000 in 2010.    The average ticket (total purchase) at domestic Starbuck stores in  No … vember 2007 was reported at $6.36.',\n",
       " 'The average ticket (total purchase) at domestic Starbuck stores in  No … vember 2007 was reported at $6.36.    In 2008, the average ticket was flat (0.0% change).In fiscal 2014, Walgreens opened a total of 184 new locations and acquired 84 locations, for a net decrease of 273 after relocations and closings. How big are your stores? The average size for a typical Walgreens is about 14,500 square feet and the sales floor averages about 11,000 square feet. How do we select locations for new stores? There are several factors that Walgreens takes into',\n",
       " 'and the sales floor averages about 11,000 square feet. How do we select locations for new stores? There are several factors that Walgreens takes into account, such as major intersections, traffic patterns, demographics and locations near hospitals.th store in 1984, reaching $4 billion in sales in 1987, and $5 billion two years later. Walgreens ended the 1980s with 1,484 stores, $5.3 billion in revenues and $154 million in profits. However, profit margins remained just below 3 percent of sales, and returns on assets of less than 10 percent.The number of Walgreen stores has risen from 5,000 in 2005 to more',\n",
       " '3 percent of sales, and returns on assets of less than 10 percent.The number of Walgreen stores has risen from 5,000 in 2005 to more than 8,000 at present. The average square footage per store stood at approximately 10,200 and we forecast the figure to remain constant over our review period. Walgreen earned $303 as average front-end revenue per store square foot in 2012.Your Walgreens Store. Select a store from the search results to make it Your Walgreens Store and save time getting what you need. Your Walgreens Store will be the default location for picking up prescriptions, photos, in',\n",
       " 'it Your Walgreens Store and save time getting what you need. Your Walgreens Store will be the default location for picking up prescriptions, photos, in store orders and finding deals in the Weekly Ad.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['chunks'] = df['contexts'].apply(lambda x: [i['text'] for i in text_to_chunks(x[0])])\n",
    "df['chunks'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    for j in df['chunks'][i]:\n",
    "        doc = Document(\n",
    "            metadata={\n",
    "                \"question\": df['question'][i],\n",
    "            },\n",
    "            page_content=j)\n",
    "        data.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialie the embedding model\n",
    "model_name = \"intfloat/e5-large-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtral_llm = bam_model(model_id=\"mistralai/mixtral-8x7b-instruct-v0-1\", repetition_penalty=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['QUESTION'] template='Please write a paragraph to answer the below question.\\nQuestion: {QUESTION}\\n'\n"
     ]
    }
   ],
   "source": [
    "# Setting up the hypothetical answer retrieval mechanism\n",
    "prompt = 'Please write a paragraph to answer the below question.\\nQuestion: {QUESTION}\\n'\n",
    "prompt_template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "\n",
    "# Generate hypothetical document for query using zero shot LLM call, and then embed that using the embeddings model defined above.\n",
    "embeddings = HypotheticalDocumentEmbedder.from_llm(mixtral_llm,\n",
    "                                                   embeddings_model,\n",
    "                                                   custom_prompt=prompt_template,\n",
    "                                                   verbose = False\n",
    "                                                   )\n",
    "print(embeddings.llm_chain.prompt)\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the data into vector store\n",
    "vectorstore = FAISS.from_documents(data, embeddings)\n",
    "\n",
    "# Retrieve similar documents to hypothetical answer from the vectorstore\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "hour between their wages and tips. According to bartending.org, bartenders in a high volume resort or establishment can earn $50,000 to $75,000 per year between hourly wages and tips. Indeed.com 2010 results show bartenders in restaurants at median salary rates can make a good salary per year: Bartender $73,000.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "$18,970 a year. The median earnings of bartenders during this period were $9.06 an hour and $18,850 a year. Eighty percent of bartenders in the U.S. reported annual incomes of between $16,170 and $31,860.Tips make up half or more of bartender's salaries. If a bartender earned $6.00 an hour, their tips generally average out to $12.00 to $18.00 an hour as additional income. A bartender in an average bar will typically earn $15.00 $30.00 an hour between their wages and tips. According to bartending.org, bartenders in a high volume resort or establishment can earn $50,000 to $75,000 per year between\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "But for the most part, bartending is almost always rewarding in the financial sense, as long as you stick with it.About 551,100 individuals are employed as bartenders, with half of this number working part-time. The average annual salary for bartenders is $19,050 or an equivalent of $9.16 per hour, including tips. No formal training is needed for one to get a job as a bartender as all it takes are good customer service skills and a comprehensive knowledge about beverages and recipes. Confidence votes 11. Bartenders in Vegas can make up to $7-$15 wage plus $10-$50 in tips per hour.\n"
     ]
    }
   ],
   "source": [
    "# Testing a sample query from the dataset\n",
    "query = \"how much do bartenders make\"\n",
    "docs = retriever.get_relevant_documents(query,verbose=False)\n",
    "\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generation(context, query):\n",
    "    template = (\n",
    "        \"<s>\"\n",
    "        \"[INST] \\n\"\n",
    "        \"Context: {context} \\n\"\n",
    "        \"- Take the context above and use that to answer questions in a detailed and professional way. \\n\"\n",
    "        \"- If you don't know the answer just say \\\"I don't know\\\".\\n\"\n",
    "        \"- Refrain from using any other knowledge other than the text provided.\\n\"\n",
    "        \"- Don't mention that you are answering from the text, impersonate as if this is coming from your knowledge\\n\"\n",
    "        \"- For the questions whose answer is not available in the provided context, just say \\\"I don't know\\\".\\n\"\n",
    "        \"Question: {query}? \\n\"\n",
    "        \"[/INST] \\n\"\n",
    "        \"</s>\\n\"\n",
    "        \"Answer: \"\n",
    "    )\n",
    "\n",
    "    qa_template = PromptTemplate.from_template(template)\n",
    "    return qa_template.format(context=context, query=query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] \n",
      "Context: hour between their wages and tips. According to bartending.org, bartenders in a high volume resort or establishment can earn $50,000 to $75,000 per year between hourly wages and tips. Indeed.com 2010 results show bartenders in restaurants at median salary rates can make a good salary per year: Bartender $73,000.\n",
      "$18,970 a year. The median earnings of bartenders during this period were $9.06 an hour and $18,850 a year. Eighty percent of bartenders in the U.S. reported annual incomes of between $16,170 and $31,860.Tips make up half or more of bartender's salaries. If a bartender earned $6.00 an hour, their tips generally average out to $12.00 to $18.00 an hour as additional income. A bartender in an average bar will typically earn $15.00 $30.00 an hour between their wages and tips. According to bartending.org, bartenders in a high volume resort or establishment can earn $50,000 to $75,000 per year between\n",
      "But for the most part, bartending is almost always rewarding in the financial sense, as long as you stick with it.About 551,100 individuals are employed as bartenders, with half of this number working part-time. The average annual salary for bartenders is $19,050 or an equivalent of $9.16 per hour, including tips. No formal training is needed for one to get a job as a bartender as all it takes are good customer service skills and a comprehensive knowledge about beverages and recipes. Confidence votes 11. Bartenders in Vegas can make up to $7-$15 wage plus $10-$50 in tips per hour. \n",
      "- Take the context above and use that to answer questions in a detailed and professional way. \n",
      "- If you don't know the answer just say \"I don't know\".\n",
      "- Refrain from using any other knowledge other than the text provided.\n",
      "- Don't mention that you are answering from the text, impersonate as if this is coming from your knowledge\n",
      "- For the questions whose answer is not available in the provided context, just say \"I don't know\".\n",
      "Question: how much do bartenders make? \n",
      "[/INST] \n",
      "</s>\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# Generate the LLM Response\n",
    "context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "prompt = prompt_generation(context, query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "Bartenders can earn a varying amount of money depending on the type of establishment they work in and their level of experience. According to the information provided, the median earnings of bartenders in the US during a certain period were $9.06 an hour and $18,850 a year. However, bartenders in high volume resort or establishments have the potential to earn a higher income, with annual salaries ranging from $50,000 to $75,000 between hourly wages and tips. On average, bartenders earn $19,050 or an equivalent of $9.16 per hour, including tips. In some places like Las Vegas, bartenders can make a higher wage of $7-$15 plus tips which can range from $10-$50 per hour. It's important to note that tips often make up half or more of a bartender's salary, with some earning an additional $12.00 to $18.00 an hour on top of their hourly wage.\n"
     ]
    }
   ],
   "source": [
    "result = mixtral_llm.invoke(prompt)\n",
    "print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_GROUNDEDNESS_FULL_SYSTEM = \"\"\"You are a INFORMATION OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.\n",
    "For every sentence in the statement, please answer with this template:\n",
    "\n",
    "TEMPLATE: \n",
    "Statement Sentence: <Sentence>\n",
    "Supporting Evidence: <Choose the exact unchanged sentences in the source that can answer the statement (Enclose them in \"), if nothing matches, say NOTHING FOUND>\n",
    "Score: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\n",
    "\"\"\"\n",
    "\n",
    "QS_RELEVANCE = \"\"\"You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
    "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
    "\n",
    "A few additional scoring guidelines:\n",
    "\n",
    "- Long STATEMENTS should score equally well as short STATEMENTS.\n",
    "\n",
    "- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.\n",
    "\n",
    "- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.\n",
    "\n",
    "- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.\n",
    "\n",
    "- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
    "\n",
    "- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.\n",
    "\n",
    "- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.\n",
    "\n",
    "- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.\n",
    "\n",
    "- Never elaborate.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "STATEMENT: {statement}\n",
    "\n",
    "RELEVANCE: \"\"\"\n",
    "\n",
    "PR_RELEVANCE = \"\"\"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\n",
    "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant. \n",
    "\n",
    "A few additional scoring guidelines:\n",
    "\n",
    "- Long RESPONSES should score equally well as short RESPONSES.\n",
    "\n",
    "- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\n",
    "\n",
    "- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\n",
    "\n",
    "- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\n",
    "\n",
    "- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\n",
    "\n",
    "- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\n",
    "\n",
    "- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
    "\n",
    "- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\n",
    "\n",
    "- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\n",
    "\n",
    "- RESPONSE that confidently FALSE should get a score of 0.\n",
    "\n",
    "- RESPONSE that is only seemingly RELEVANT should get a score of 0.\n",
    "\n",
    "- Never elaborate.\n",
    "\n",
    "PROMPT: {prompt}\n",
    "\n",
    "RESPONSE: {response}\n",
    "\n",
    "RELEVANCE: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "from trulens_eval import TruCustomApp\n",
    "from trulens_eval import Feedback, Select\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider.langchain import Langchain\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "from trulens_eval.feedback import prompts\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self._retriever = retriever\n",
    "        self._llm = llm\n",
    "    \n",
    "    @instrument\n",
    "    def retrieve_chunks(self, query, num_chunks):\n",
    "        self._retriever.search_kwargs = {\"k\": num_chunks}\n",
    "        docs = self._retriever.get_relevant_documents(query)\n",
    "        docs = [doc.page_content for doc in docs]\n",
    "        return docs\n",
    "    \n",
    "    @instrument\n",
    "    def join_chunks(self, chunks):\n",
    "        return \"\\n\".join(chunks)\n",
    "    \n",
    "    @instrument\n",
    "    def respond_to_query(self, query, num_chunks=3):\n",
    "        chunks = self.retrieve_chunks(query, num_chunks=num_chunks)\n",
    "        context = self.join_chunks(chunks)\n",
    "        prompt = prompt_generation(context, query)\n",
    "        retries_left = 3\n",
    "        answer = ''\n",
    "        while True:\n",
    "            try:\n",
    "                answer = self._llm.invoke(prompt).strip()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Error while generating answer\", e)\n",
    "                if retries_left > 0:\n",
    "                    retries_left -= 1\n",
    "                    print(\"Retrying. Retries Remaining -\", retries_left)\n",
    "                else:\n",
    "                    raise\n",
    "        return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_rag = Processor(retriever, mixtral_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "class IBMLangchain(Langchain):\n",
    "    def _create_chat_completion(self, prompt = None, messages = None, **kwargs):\n",
    "        if prompt is not None:\n",
    "            # prompt += \"\\nANSWER:\\n\"\n",
    "            prompt = f\"[INST]\\n{prompt}\\n[/INST]\"\n",
    "            predict = self.endpoint.chain.invoke(prompt, **kwargs)\n",
    "            predict = re.sub(r'Score: (\\d+)/\\d+', r'Score: \\1', predict)\n",
    "\n",
    "        elif messages is not None:\n",
    "            prompt = messages[0]['content']\n",
    "            # prompt += \"\\nANSWER:\\n\"\n",
    "            prompt = f\"[INST]\\n{prompt}\\n[/INST]\"\n",
    "            predict = self.endpoint.chain.invoke(prompt, **kwargs)\n",
    "            predict = re.sub(r'Score: (\\d+)/\\d+', r'Score: \\1', predict)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"`prompt` or `messages` must be specified.\")\n",
    "        \n",
    "        return predict\n",
    "    \n",
    "    def _groundedness_doc_in_out(self, premise: str, hypothesis: str) -> str:\n",
    "        \"\"\"\n",
    "        An LLM prompt using the entire document for premise and entire statement\n",
    "        document for hypothesis.\n",
    "\n",
    "        Args:\n",
    "            premise (str): A source document\n",
    "            hypothesis (str): A statement to check\n",
    "\n",
    "        Returns:\n",
    "            str: An LLM response using a scorecard template\n",
    "        \"\"\"\n",
    "        assert self.endpoint is not None, \"Endpoint is not set.\"\n",
    "\n",
    "        return self.endpoint.run_in_pace(\n",
    "            func=self._create_chat_completion,\n",
    "            prompt=str.format(LLM_GROUNDEDNESS_FULL_SYSTEM,) +\n",
    "            str.format(\n",
    "                prompts.LLM_GROUNDEDNESS_FULL_PROMPT,\n",
    "                premise=premise,\n",
    "                hypothesis=hypothesis\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_llm = bam_model(model_id=\"mistralai/mixtral-8x7b-instruct-v0-1\", min_new_tokens=100, max_new_tokens=1000, repetition_penalty=1.1)\n",
    "# eval_llm = bam_model(model_id=\"meta-llama/llama-2-70b-chat\", max_new_tokens=1000, repetition_penalty=1.1)\n",
    "\n",
    "langchain_provider = IBMLangchain(chain=eval_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 Tru initialized with db url sqlite:///default.sqlite .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input context will be set to __record__.app.retrieve_chunks.rets[:] .\n",
      "✅ In Groundedness, input source will be set to __record__.app.join_chunks.rets .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_qs_relevance = (\n",
    "    Feedback(\n",
    "        langchain_provider.qs_relevance_with_cot_reasons,\n",
    "        name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input()\n",
    "    .on(Select.RecordCalls.retrieve_chunks.rets[:])\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "grounded = Groundedness(groundedness_provider=langchain_provider)\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        grounded.groundedness_measure_with_cot_reasons,\n",
    "        name=\"Groundedness\"\n",
    "    )\n",
    "    .on(Select.RecordCalls.join_chunks.rets) # collect context chunks into a list\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(\n",
    "    langchain_provider.relevance_with_cot_reasons,\n",
    "    name=\"Answer Relevance\"\n",
    ").on_input().on_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_rag = TruCustomApp(hyde_rag,\n",
    "    app_id = 'HyDE RAG Pipeline',\n",
    "    feedbacks = [f_qs_relevance, f_groundedness, f_qa_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3900832c29e4088a6e3aa9d5f266107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://9.199.156.133:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 134/200 [31:19<17:59, 16.36s/it]Validation error: 1 validation error for Rating\n",
      "rating\n",
      "  Value error, Rating must be between 0 and 10 [type=value_error, input_value=11, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/value_error\n",
      " 92%|█████████▏| 183/200 [43:28<04:46, 16.88s/it]/Users/sourav/workstuffs/env_exp/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/sourav/workstuffs/env_exp/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 96%|█████████▋| 193/200 [45:58<01:50, 15.76s/it]/Users/sourav/workstuffs/env_exp/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/sourav/workstuffs/env_exp/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 200/200 [47:44<00:00, 14.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "with tru_rag as recording:\n",
    "    for query in tqdm(df[\"question\"], total=len(df)):\n",
    "        ans = hyde_rag.respond_to_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HyDE RAG Pipeline</th>\n",
       "      <td>0.741183</td>\n",
       "      <td>0.827197</td>\n",
       "      <td>0.990594</td>\n",
       "      <td>13.678218</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Groundedness  Context Relevance  Answer Relevance  \\\n",
       "app_id                                                                 \n",
       "HyDE RAG Pipeline      0.741183           0.827197          0.990594   \n",
       "\n",
       "                     latency  total_cost  \n",
       "app_id                                    \n",
       "HyDE RAG Pipeline  13.678218         0.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "context_compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
