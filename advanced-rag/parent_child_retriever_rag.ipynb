{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval import TruCustomApp\n",
    "from trulens_eval import Feedback, Select\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider.langchain import Langchain\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "from trulens_eval.feedback import prompts\n",
    "import custom_prompts\n",
    "\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    TextGenerationParameters,\n",
    ")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bam_model(model_id='mistralai/mixtral-8x7b-instruct-v0-1', decoding_method='greedy', max_new_tokens=1000, \n",
    "              min_new_tokens=1, temperature=0.5, top_k=50, top_p=1, repetition_penalty=1):\n",
    "\n",
    "    if decoding_method == 'greedy':\n",
    "        decoding_method = DecodingMethod.GREEDY\n",
    "        parameters=TextGenerationParameters(\n",
    "            decoding_method=decoding_method,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            min_new_tokens=min_new_tokens,\n",
    "            repetition_penalty=repetition_penalty\n",
    "        )\n",
    "    else:\n",
    "        decoding_method = DecodingMethod.SAMPLE\n",
    "        parameters=TextGenerationParameters(\n",
    "            decoding_method=decoding_method,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            min_new_tokens=min_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty\n",
    "        )\n",
    "\n",
    "    llm = LangChainInterface(\n",
    "        model_id=model_id,\n",
    "        client=Client(credentials=Credentials.from_env()),\n",
    "        parameters=parameters,\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtral_llm = bam_model(model_id=\"mistralai/mixtral-8x7b-instruct-v0-1\", repetition_penalty=1.1)\n",
    "\n",
    "# prompt = \"Tell me about IBM.\"\n",
    "# result = mixtral_llm.invoke(prompt)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walgreens store sales average</td>\n",
       "      <td>The average Walgreens salary ranges from appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how much do bartenders make</td>\n",
       "      <td>A bartender’s income is comprised mostly of ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is a furuncle boil</td>\n",
       "      <td>Knowledge center. A boil, also known as a furu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what can urinalysis detect</td>\n",
       "      <td>Urinalysis: One way to test for bladder cancer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is vitamin a used for</td>\n",
       "      <td>Since vitamin A is fat-soluble it is not neede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>who wrote Nothing compares to you</td>\n",
       "      <td>Nothing Compares 2 U  is a song originally wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>cost for relining dentures</td>\n",
       "      <td>When dentures that used to fit are now loose a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>what is sherry wine made of</td>\n",
       "      <td>Sherry vinegar is made from sherry, a fortifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>dna in bacteria</td>\n",
       "      <td>Bacterial DNA in Human Genomes. A new study fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>primary secondary and tertiary sectors definition</td>\n",
       "      <td>The movement of goods and services through the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0                        walgreens store sales average   \n",
       "1                          how much do bartenders make   \n",
       "2                              what is a furuncle boil   \n",
       "3                           what can urinalysis detect   \n",
       "4                           what is vitamin a used for   \n",
       "..                                                 ...   \n",
       "195                  who wrote Nothing compares to you   \n",
       "196                         cost for relining dentures   \n",
       "197                        what is sherry wine made of   \n",
       "198                                    dna in bacteria   \n",
       "199  primary secondary and tertiary sectors definition   \n",
       "\n",
       "                                              contexts  \n",
       "0    The average Walgreens salary ranges from appro...  \n",
       "1    A bartender’s income is comprised mostly of ti...  \n",
       "2    Knowledge center. A boil, also known as a furu...  \n",
       "3    Urinalysis: One way to test for bladder cancer...  \n",
       "4    Since vitamin A is fat-soluble it is not neede...  \n",
       "..                                                 ...  \n",
       "195  Nothing Compares 2 U  is a song originally wri...  \n",
       "196  When dentures that used to fit are now loose a...  \n",
       "197  Sherry vinegar is made from sherry, a fortifie...  \n",
       "198  Bacterial DNA in Human Genomes. A new study fi...  \n",
       "199  The movement of goods and services through the...  \n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ms-marco.csv\")\n",
    "df[\"contexts\"] = df[\"contexts\"].apply(lambda x: ast.literal_eval(x)[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialie the embedding model\n",
    "model_name = \"intfloat/e5-large-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager. Salary information comes from 7,810 data points collected directly from employees, users, and jobs on Indeed.The average revenue in 2011 of a Starbuck Store was $1,078,000, up  from $1,011,000 in 2010.    The average ticket (total purchase) at domestic Starbuck stores in  No … vember 2007 was reported at $6.36.    In 2008, the average ticket was flat (0.0% change).In fiscal 2014, Walgreens opened a total of 184 new locations and acquired 84 locations, for a net decrease of 273 after relocations and closings. How big are your stores? The average size for a typical Walgreens is about 14,500 square feet and the sales floor averages about 11,000 square feet. How do we select locations for new stores? There are several factors that Walgreens takes into account, such as major intersections, traffic patterns, demographics and locations near hospitals.th store in 1984, reaching $4 billion in sales in 1987, and $5 billion two years later. Walgreens ended the 1980s with 1,484 stores, $5.3 billion in revenues and $154 million in profits. However, profit margins remained just below 3 percent of sales, and returns on assets of less than 10 percent.The number of Walgreen stores has risen from 5,000 in 2005 to more than 8,000 at present. The average square footage per store stood at approximately 10,200 and we forecast the figure to remain constant over our review period. Walgreen earned $303 as average front-end revenue per store square foot in 2012.Your Walgreens Store. Select a store from the search results to make it Your Walgreens Store and save time getting what you need. Your Walgreens Store will be the default location for picking up prescriptions, photos, in store orders and finding deals in the Weekly Ad.', metadata={'question': 'walgreens store sales average'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(len(df)):\n",
    "    doc = Document(\n",
    "        metadata={\n",
    "            \"question\": df['question'][i],\n",
    "        },\n",
    "        page_content=df['contexts'][i])\n",
    "    data.append(doc)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"text1\", \"text2\", \"text3\"]\n",
    "faiss = FAISS.from_texts(texts, embeddings_model)\n",
    "\n",
    "# Define the child and parent splitters\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=25)\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=75)\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Initialize the ParentDocumentRetriever with FAISS\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "  vectorstore=faiss,\n",
    "  docstore=store,\n",
    "  child_splitter=child_splitter,\n",
    "  parent_splitter=parent_splitter\n",
    ")\n",
    "\n",
    "# Add documents to the retriever\n",
    "parent_document_retriever.add_documents(data, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generation(context, query):\n",
    "    template = (\n",
    "        \"<s>\"\n",
    "        \"[INST] \\n\"\n",
    "        \"Context: {context} \\n\"\n",
    "        \"- Take the context above and use that to answer questions in a detailed and professional way. \\n\"\n",
    "        \"- If you don't know the answer just say \\\"I don't know\\\".\\n\"\n",
    "        \"- Refrain from using any other knowledge other than the text provided.\\n\"\n",
    "        \"- Don't mention that you are answering from the text, impersonate as if this is coming from your knowledge\\n\"\n",
    "        \"- For the questions whose answer is not available in the provided context, just say \\\"I don't know\\\".\\n\"\n",
    "        \"Question: {query}? \\n\"\n",
    "        \"[/INST] \\n\"\n",
    "        \"</s>\\n\"\n",
    "        \"Answer: \"\n",
    "    )\n",
    "\n",
    "    qa_template = PromptTemplate.from_template(template)\n",
    "    return qa_template.format(context=context, query=query)\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "the highest incomes in 2011, an average of $26,180 a year. Bartenders employed in full-service restaurants tended to earn somewhat less, averaging about $22,130 a year. Bartenders employed by bars earned an average of $20,230 per year, and bartenders who worked for civic and social organizations\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "night total (on a good night)...just depends.Report Abuse. no way to tell you how much bartenders make. in wages anything from minimum to $15 an hour. tips, anywhere from $20 to $300 or more a night. depends on a lot of things. those top dollar jobs only come after a lot of experience. If the bar\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "of bartenders in the U.S. reported annual incomes of between $16,170 and $31,860.Tips make up half or more of bartender's salaries. If a bartender earned $6.00 an hour, their tips generally average out to $12.00 to $18.00 an hour as additional income. A bartender in an average bar will typically\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "wants to make. I wouldn't say there is a cap on how much a bartender make in one year. If the service is good and the coversation is i … nteresting a bartender can make alot of money. Location is also a big factor.Best Answer: An average bartender makes about...2 to 3 dollars an hour. but all the\n"
     ]
    }
   ],
   "source": [
    "query= \"how much do bartenders make\"\n",
    "docs = parent_document_retriever.get_relevant_documents(query)\n",
    "\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='the highest incomes in 2011, an average of $26,180 a year. Bartenders employed in full-service restaurants tended to earn somewhat less, averaging about $22,130 a year. Bartenders employed by bars earned an average of $20,230 per year, and bartenders who worked for civic and social organizations', metadata={'question': 'how much do bartenders make'}),\n",
       " Document(page_content='night total (on a good night)...just depends.Report Abuse. no way to tell you how much bartenders make. in wages anything from minimum to $15 an hour. tips, anywhere from $20 to $300 or more a night. depends on a lot of things. those top dollar jobs only come after a lot of experience. If the bar', metadata={'question': 'how much do bartenders make'}),\n",
       " Document(page_content=\"of bartenders in the U.S. reported annual incomes of between $16,170 and $31,860.Tips make up half or more of bartender's salaries. If a bartender earned $6.00 an hour, their tips generally average out to $12.00 to $18.00 an hour as additional income. A bartender in an average bar will typically\", metadata={'question': 'how much do bartenders make'}),\n",
       " Document(page_content=\"wants to make. I wouldn't say there is a cap on how much a bartender make in one year. If the service is good and the coversation is i … nteresting a bartender can make alot of money. Location is also a big factor.Best Answer: An average bartender makes about...2 to 3 dollars an hour. but all the\", metadata={'question': 'how much do bartenders make'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] \n",
      "Context: the highest incomes in 2011, an average of $26,180 a year. Bartenders employed in full-service restaurants tended to earn somewhat less, averaging about $22,130 a year. Bartenders employed by bars earned an average of $20,230 per year, and bartenders who worked for civic and social organizations\n",
      "night total (on a good night)...just depends.Report Abuse. no way to tell you how much bartenders make. in wages anything from minimum to $15 an hour. tips, anywhere from $20 to $300 or more a night. depends on a lot of things. those top dollar jobs only come after a lot of experience. If the bar\n",
      "of bartenders in the U.S. reported annual incomes of between $16,170 and $31,860.Tips make up half or more of bartender's salaries. If a bartender earned $6.00 an hour, their tips generally average out to $12.00 to $18.00 an hour as additional income. A bartender in an average bar will typically\n",
      "wants to make. I wouldn't say there is a cap on how much a bartender make in one year. If the service is good and the coversation is i … nteresting a bartender can make alot of money. Location is also a big factor.Best Answer: An average bartender makes about...2 to 3 dollars an hour. but all the \n",
      "- Take the context above and use that to answer questions in a detailed and professional way. \n",
      "- If you don't know the answer just say \"I don't know\".\n",
      "- Refrain from using any other knowledge other than the text provided.\n",
      "- Don't mention that you are answering from the text, impersonate as if this is coming from your knowledge\n",
      "- For the questions whose answer is not available in the provided context, just say \"I don't know\".\n",
      "Question: how much do bartenders make? \n",
      "[/INST] \n",
      "</s>\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "prompt = prompt_generation(context, query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "Bartenders can make varying amounts depending on several factors such as their employer and level of experience. On average, bartenders earn between $16,170 and $31,860 per year, with tips making up at least half of their salary. The hourly wage for a bartender can range from minimum wage to around $15, with tips potentially adding another $12 to $18 per hour. It's important to note that high-earning bartenders can make significantly more than this, especially in busy locations or with excellent customer service skills. However, without specific information about the situation, it's difficult to provide an exact figure for how much a bartender makes.\n"
     ]
    }
   ],
   "source": [
    "result = mixtral_llm.invoke(prompt)\n",
    "print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self._retriever = retriever\n",
    "        self._llm = llm\n",
    "    \n",
    "    @instrument\n",
    "    def retrieve_chunks(self, query, num_chunks):\n",
    "        self._retriever.search_kwargs = {\"k\": num_chunks}\n",
    "        docs = self._retriever.get_relevant_documents(query)\n",
    "        docs = [doc.page_content for doc in docs]\n",
    "        return docs\n",
    "    \n",
    "    @instrument\n",
    "    def join_chunks(self, chunks):\n",
    "        return \"\\n\".join(chunks)\n",
    "    \n",
    "    @instrument\n",
    "    def respond_to_query(self, query, num_chunks=3):\n",
    "        chunks = self.retrieve_chunks(query, num_chunks=num_chunks)\n",
    "        context = self.join_chunks(chunks)\n",
    "        prompt = prompt_generation(context, query)\n",
    "        retries_left = 3\n",
    "        while True:\n",
    "            try:\n",
    "                answer = self._llm.invoke(prompt).strip()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Error while generating answer\", e)\n",
    "                if retries_left>0:\n",
    "                    retries_left -= 1\n",
    "                    print(\"Retrying. Retries Remaining -\", retries_left)\n",
    "                else:\n",
    "                    raise\n",
    "        return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_doc_rag = Processor(parent_document_retriever, mixtral_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBMLangchain(Langchain):\n",
    "    def _create_chat_completion(self, prompt = None, messages = None, **kwargs):\n",
    "        if prompt is not None:\n",
    "            # prompt += \"\\nANSWER:\\n\"\n",
    "            prompt = f\"[INST]\\n{prompt}\\n[/INST]\"\n",
    "            predict = self.endpoint.chain.invoke(prompt, **kwargs)\n",
    "            predict = re.sub(r'Score: (\\d+)/\\d+', r'Score: \\1', predict)\n",
    "\n",
    "        elif messages is not None:\n",
    "            prompt = messages[0]['content']\n",
    "            # prompt += \"\\nANSWER:\\n\"\n",
    "            prompt = f\"[INST]\\n{prompt}\\n[/INST]\"\n",
    "            predict = self.endpoint.chain.invoke(prompt, **kwargs)\n",
    "            predict = re.sub(r'Score: (\\d+)/\\d+', r'Score: \\1', predict)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"`prompt` or `messages` must be specified.\")\n",
    "        \n",
    "        return predict\n",
    "    \n",
    "    def _groundedness_doc_in_out(self, premise: str, hypothesis: str) -> str:\n",
    "        \"\"\"\n",
    "        An LLM prompt using the entire document for premise and entire statement\n",
    "        document for hypothesis.\n",
    "\n",
    "        Args:\n",
    "            premise (str): A source document\n",
    "            hypothesis (str): A statement to check\n",
    "\n",
    "        Returns:\n",
    "            str: An LLM response using a scorecard template\n",
    "        \"\"\"\n",
    "        assert self.endpoint is not None, \"Endpoint is not set.\"\n",
    "\n",
    "        return self.endpoint.run_in_pace(\n",
    "            func=self._create_chat_completion,\n",
    "            prompt=str.format(custom_prompts.LLM_GROUNDEDNESS_FULL_SYSTEM,) +\n",
    "            str.format(\n",
    "                prompts.LLM_GROUNDEDNESS_FULL_PROMPT,\n",
    "                premise=premise,\n",
    "                hypothesis=hypothesis\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_llm = bam_model(model_id=\"mistralai/mixtral-8x7b-instruct-v0-1\", min_new_tokens=1, max_new_tokens=1000, repetition_penalty=1.1)\n",
    "# eval_llm = bam_model(model_id=\"meta-llama/llama-2-70b-chat\", max_new_tokens=1000, repetition_penalty=1.1)\n",
    "\n",
    "langchain_provider = IBMLangchain(chain=eval_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input context will be set to __record__.app.retrieve_chunks.rets[:] .\n",
      "✅ In Groundedness, input source will be set to __record__.app.join_chunks.rets .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# Question/statement relevance between question and each context chunk.\n",
    "f_qs_relevance = (\n",
    "    Feedback(\n",
    "        langchain_provider.qs_relevance_with_cot_reasons,\n",
    "        name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input()\n",
    "    .on(Select.RecordCalls.retrieve_chunks.rets[:])\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "grounded = Groundedness(groundedness_provider=langchain_provider)\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        grounded.groundedness_measure_with_cot_reasons,\n",
    "        name=\"Groundedness\"\n",
    "    )\n",
    "    .on(Select.RecordCalls.join_chunks.rets) # collect context chunks into a list\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(\n",
    "    langchain_provider.relevance_with_cot_reasons,\n",
    "    name=\"Answer Relevance\"\n",
    ").on_input().on_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_rag = TruCustomApp(parent_doc_rag,\n",
    "    app_id = 'Parent doc retrieval RAG Pipeline',\n",
    "    feedbacks = [f_qs_relevance, f_groundedness, f_qa_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 127/200 [13:06<08:06,  6.67s/it]/Users/sourav/workstuffs/env_exp/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/sourav/workstuffs/env_exp/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 74%|███████▍  | 148/200 [15:40<08:07,  9.37s/it]/Users/sourav/workstuffs/env_exp/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/sourav/workstuffs/env_exp/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 80%|████████  | 160/200 [16:55<03:52,  5.82s/it]/Users/sourav/workstuffs/env_exp/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/sourav/workstuffs/env_exp/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 200/200 [21:40<00:00,  6.50s/it]\n"
     ]
    }
   ],
   "source": [
    "with tru_rag as recording:\n",
    "    for query in tqdm(df[\"question\"], total=len(df)):\n",
    "        ans = parent_doc_rag.respond_to_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Parent doc retrieval RAG Pipeline</th>\n",
       "      <td>0.98191</td>\n",
       "      <td>0.708164</td>\n",
       "      <td>0.721465</td>\n",
       "      <td>5.795</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Answer Relevance  Groundedness  \\\n",
       "app_id                                                              \n",
       "Parent doc retrieval RAG Pipeline           0.98191      0.708164   \n",
       "\n",
       "                                   Context Relevance  latency  total_cost  \n",
       "app_id                                                                     \n",
       "Parent doc retrieval RAG Pipeline           0.721465    5.795         0.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
